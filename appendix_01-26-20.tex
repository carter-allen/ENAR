\documentclass[useAMS,11pt]{article}

%%%%%%%%%%%%
% Packages %
%%%%%%%%%%%%
\usepackage[margin=0.5in]{geometry}
\usepackage{lineno,hyperref}
\usepackage{amsmath,amssymb,amsfonts,dsfont}
\usepackage{graphicx}
\usepackage{bbm}
\usepackage{booktabs}
\usepackage{caption}
\usepackage[large, bf]{subfigure}
\usepackage[compact]{titlesec}
\usepackage{bm}
\usepackage{multirow}
\usepackage{rotating}
\usepackage{pdflscape}


%%%%%%%%%%%%
% Commands %
%%%%%%%%%%%%
\newcommand{\bfa} {\boldsymbol{\alpha}}
\newcommand{\bfA}{\boldsymbol{A}}
\newcommand{\bfb}{\boldsymbol{\beta}}
\newcommand{\bfbe} {\boldsymbol{b}}
\newcommand{\bfbio} {\boldsymbol{b}_{1i}}
\newcommand{\bfbit} {\boldsymbol{b}_{2i}}
\newcommand{\bfC} {\boldsymbol{C}}
\newcommand{\bfD} {\boldsymbol{D}}
\renewcommand{\d}{\text{d}}
\newcommand{\e}{\text{e}}
\newcommand{\E}{\text{E}}
\newcommand{\ea}{\e^{\bfxi'\bfa}}
\newcommand{\bfe} {\mbox{\boldmath $\eta$}}
\newcommand{\bfg} {\mbox{\boldmath $\gamma$}}
\newcommand{\bfG} {\mbox{\boldmath $\Gamma$}}
\newcommand{\bfI} {\boldsymbol{I}}
\newcommand{\bfl} {\boldsymbol{\lambda}}
\newcommand{\bflj} {\boldsymbol{\lambda}_j}
\newcommand{\bfL} {\boldsymbol{\Lambda}}
\newcommand{\bfem}{\boldsymbol{m}}
\newcommand{\bfm} {\boldsymbol{\mu}}
\newcommand{\bfM}{\boldsymbol{M}}
\newcommand{\N}{\text{N}}
\newcommand{\bfO} {\boldsymbol{\Omega}}
\newcommand{\bfpi} {\boldsymbol{\pi}}
\newcommand{\bfpsi} {\boldsymbol{\psi}}
\newcommand{\psix} {\psi(\bfx_i)}
\newcommand{\bfPsi} {\boldsymbol{\Psi}}
\newcommand{\bfp}{\boldsymbol{\phi}}
\newcommand{\bfP}{\boldsymbol{\Phi}}
\newcommand{\bfpb}{\bar{\bfp}}
\newcommand{\bfpm}{\bfp_{(-i)}}
\newcommand{\Q}{\partial}
\newcommand{\bfQ}{\boldsymbol{Q}}
\newcommand{\bfr} {\boldsymbol{\rho}}
\newcommand{\bfR}{\boldsymbol{R}}
\newcommand{\bfes} {\boldsymbol{s}}
\newcommand{\bfeS} {\boldsymbol{S}}
\newcommand{\bfs} {\boldsymbol{\sigma}}
\newcommand{\bfS} {\boldsymbol{\Sigma}}
\newcommand{\bfse}{\boldsymbol{\Sigma}_e}
\newcommand{\bfsp}{\boldsymbol{\Sigma}_{\phi}}
\newcommand{\bfsa}{\boldsymbol{\Sigma}_{\alpha}}
\newcommand{\bfsb}{\boldsymbol{\Sigma}_{\beta}}
\newcommand{\bfta} {\boldsymbol{\tau}}
\newcommand{\ta}{\text{*}}
\newcommand{\bft} {\boldsymbol{\theta}}
\newcommand{\bfT} {\boldsymbol{\Theta}}
\newcommand{\bftit} {\boldsymbol{t}'_i}
\newcommand{\bfu} {\boldsymbol{u}}
\newcommand{\bfU} {\boldsymbol{U}}
\newcommand{\bfv} {\boldsymbol{v}}
%\newcommand{\bfV} {\boldsymbol{V}}
\newcommand{\V}{\text{V}}
\newcommand{\bfw} {\boldsymbol{w}}
\newcommand{\bfW} {\boldsymbol{W}}
\newcommand{\bfwi} {\boldsymbol{w}_i}
\newcommand{\bfwij} {\boldsymbol{w}_{ij}}
\newcommand{\bfwijt} {\boldsymbol{w}'_{ij}}
\newcommand{\bfx} {\boldsymbol{x}}
\newcommand{\bfX} {\boldsymbol{X}}
%\newcommand{\bfxi} {\boldsymbol{x}_i}
\newcommand{\bfxi} {\boldsymbol{\xi}}
\newcommand{\bfxit} {\boldsymbol{x}'_i}
\newcommand{\bfxij} {\boldsymbol{x}_{ij}}
\newcommand{\bfxijt} {\boldsymbol{x}'_{ij}}
\newcommand{\bfy} {\boldsymbol{y}}
\newcommand{\bfY} {\boldsymbol{Y}}
\newcommand{\Ystar} {Y^{\ta}}
\newcommand{\bfYstar} {\bfY^{\ta}}
\newcommand{\Yij} {\ensuremath{Y_{ij}}}
\newcommand{\bfz} {\boldsymbol{z}}
\newcommand{\bfzij} {\boldsymbol{z}_{ij}}
\newcommand{\bfZ} {\boldsymbol{Z}}
%\newcommand{\bfZb}{\boldsymbol{Z}_{\beta}}
%\newcommand{\zoi} {z_{1i}}
%\newcommand{\zti} {z_{2i}}
%\newcommand{\zso} {z^*_{1i}}
%\newcommand{\zst} {z^*_{2i}}
%\newcommand{\bfZe}{\boldsymbol{Z}_e}
\newcommand{\ti}{\text{I}}
\newcommand{\1}{\mathbbm{1}}
\newcommand{\bfzero}{\boldsymbol{0}}

\newcommand{\be}{\begin{eqnarray}}
\newcommand{\ee}{\end{eqnarray}}
\newcommand{\bee}{\begin{eqnarray*}}
\newcommand{\eee}{\end{eqnarray*}}
\newcommand{\bi}{\begin{enumerate}}
\newcommand{\ei}{\end{enumerate}}

\modulolinenumbers[5]

\renewcommand{\figurename}{\bf Web Figure}
\setcounter{figure}{0}

\renewcommand{\tablename}{\bf Web Table}
\setcounter{table}{0}

\begin{document}

\begin{center}
{\bf \Large Web-based supplementary materials for ``A Bayesian multivariate mixture model for skewed longitudinal data with intermittent missing observations: An application to infant motor development"}
\\[24pt]
{\bf \large Carter Allen$^1$, Sara E. Benjamin-Neelon$^2$ and Brian Neelon$^{3*}$}\\[12pt]
1: Department of Biomedical Informatics, The Ohio State University, Columbus, OH, U.S.A. \\[12pt]
2: Department of Health, Behavior and Society, Johns Hopkins University, Baltimore, MD, U.S.A. \\[12pt]
3: Medical University of South Carolina, Charleston, South Carolina, U.S.A.\\[12pt]
$^*$email: neelon@musc.edu\\[24pt]
\end{center}

\section*{Web Appendix A: Proof of Proposition 1}

For each cluster $k = 1,...,K$, let $\mathbf{Y}_k = \mathbf{X}^*_k \mathbf{B}^*_k + \mathbf{E}_k$, where $\mathbf{Y}_k$ is an $n_k \times J$ response matrix, $\mathbf{X}^*_k$ is an $n_k \times (p+1)$ matrix of predictors and latent truncated normal random variables from equation (7) in the manuscript, $\mathbf{B}^*_k$ is a $(p+1) \times J$ matrix of regression and skewness coefficients from equation (7), and $\mathbf{E}_k$ is the $n_k \times J$ matrix of residuals associated with $\mathbf{Y}_k$. We assume $\mathbf{E}_k \sim \text{MatNorm}(\mathbf{0},\mathbf{I}_{n_k},\boldsymbol\Sigma_k)$, where $\mathbf{0}$ is an $n_k \times J$ matrix of 0's,  $\mathbf{I}_{n_k}$ is the $n_k$ dimensional identity matrix, and $\boldsymbol\Sigma_k$ is a $J \times J$ variance-covariance matrix.
As prior distributions, we assume $\mathbf{B}^*_k|\boldsymbol\Sigma_k \sim \text{MatNorm}(\mathbf{B}^*_{0k},\mathbf{L}_{0k},\boldsymbol\Sigma_k)$ and $\boldsymbol\Sigma_k \sim \text{IW}(\nu_{0k},\mathbf{V}_{0k})$. That is,  $\mathbf{B}^*_k$ and $\boldsymbol\Sigma_k$ have a joint Matrix Normal--Inverse Wishart (IW) prior, denoted MatNorm--IW$_{(p+1)\times J}(\mathbf{B}^*_{0k},\mathbf{L}_{0k},\nu_{0k},\mathbf{V}_{0k})$, of the form
\begin{eqnarray*}
\pi(\mathbf{B}^*_k,\boldsymbol\Sigma_k)&=&\pi(\mathbf{B}^*_k|\boldsymbol\Sigma_k)\pi(\boldsymbol\Sigma_k)\\
&=&\text{MatNorm}_{(p+1)\times J}(\mathbf{B}^*_{0k},\mathbf{L}_{0k},\boldsymbol\Sigma_k)\text{IW}(\nu_{0k},\mathbf{V}_{0k}),
\end{eqnarray*}
where $\mathbf{B}^*_{0k}$ is a $(p+1)\times J$ prior mean matrix, $\mathbf{L}_{0k}$ and $\mathbf{V}_{0k}$ are, respectively, $(p+1)\times(p+1)$ and $J\times J$ prior scale matrices, and $\nu_{0k}$ denotes the prior degrees of freedom.
Under this set-up, the full conditional distribution for $\mathbf{B}^*_k$ can be obtained as follows:
\begin{eqnarray*}
	\mathbf{B}^*_k|\boldsymbol\Sigma_k,\mathbf{Y}_k & \propto & \exp \left \{-\frac{1}{2} \left ( \text{tr}[\boldsymbol\Sigma_k^{-1}(\mathbf{Y}_k - \mathbf{X}^*_k \mathbf{B}^*_k)^T (\mathbf{Y}_k - \mathbf{X}^*_k \mathbf{B}^*_k)] + \text{tr}[\boldsymbol\Sigma_k^{-1}(\mathbf{B}^*_k - \mathbf{B}^*_{0k})^T \mathbf{L}_{0k}^{-1} (\mathbf{B}^*_k - \mathbf{B}^*_{0k})] \right ) \right \} \\
	& \propto & \exp \left \{- \frac{1}{2} \left( \text{tr}[\boldsymbol\Sigma_k^{-1}(\mathbf{Y}_k - \mathbf{X}^*_k \mathbf{B}^*_k)^T (\mathbf{Y}_k - \mathbf{X}^*_k \mathbf{B}^*_k) + (\mathbf{B}^*_k - \mathbf{B}^*_{0k})^T \mathbf{L}_{0k}^{-1} (\mathbf{B}^*_k - \mathbf{B}^*_{0k})]\right ) \right \} \\
	& \propto & \exp \left \{- \frac{1}{2} \left( \text{tr}[\boldsymbol\Sigma_k^{-1}(\mathbf{B}^*_k - \mathbb{B}_k^*)^T \mathbf{L}_k^{-1}(\mathbf{B}^*_k - \mathbb{B}_k^*)] \right) \right \} \ \text{after completing the square},
\end{eqnarray*}
where $\mathbb{B}^*_k = \mathbf{L}_k(\mathbf{L}^{-1}_{0k} \mathbf{B}^*_{0k}+\mathbf{X}^{*T}_k \mathbf{Y}_k)$ and $\mathbf{L}_k = (\mathbf{L}^{-1}_{0k}+\mathbf{X}^{*T}_k \mathbf{X}^*_k)^{-1}$.
Similarly, we may express $f(\boldsymbol\Sigma_k|\mathbf{B}^*_k,\mathbf{Y}_k)$ as
\begin{eqnarray*}
	f(\boldsymbol\Sigma_k|\mathbf{B}^*_k,\mathbf{Y}_k) & \propto & f(\mathbf{Y}_k|\mathbf{B}^*_k,\boldsymbol\Sigma_k)\pi(\mathbf{B}^*_k|\boldsymbol\Sigma_k)\pi(\boldsymbol\Sigma_k), \ \text{where} \\
	f(\mathbf{Y}_k|\mathbf{B}^*_k,\boldsymbol\Sigma_k) & \propto & |\boldsymbol\Sigma_k|^{-n_k/2} \exp \left \{-\frac{1}{2} \left ( \text{tr}[\boldsymbol\Sigma_k^{-1}(\mathbf{Y}_k - \mathbf{X}^*_k \mathbf{B}^*_k)^T (\mathbf{Y}_k - \mathbf{X}^*_k \mathbf{B}^*_k)] \right ) \right \}, \\
	\pi(\mathbf{B}^*_k|\boldsymbol\Sigma_k) & \propto & |\boldsymbol\Sigma_k|^{-(p+1)/2} \exp \left \{ -\frac{1}{2} \left (\text{tr} [\boldsymbol\Sigma_k^{-1}(\mathbf{B}^*_k - \mathbf{B}^*_{0k})^T \mathbf{L}_{0k}^{-1} (\mathbf{B}^*_k - \mathbf{B}^*_{0k})] \right ) \right \}, \ \text{and} \\
	\pi(\boldsymbol\Sigma_k) & \propto & |\boldsymbol\Sigma_k|^{-(\nu_{0k}+J)/2} \exp \left \{ -\frac{1}{2} \text{tr}(\boldsymbol\Sigma_k^{-1} \mathbf{V}_{0k}) \right \}.
\end{eqnarray*}	
Combining terms, we have
\small
\begin{eqnarray*}
	f(\boldsymbol\Sigma_k|\mathbf{B}^*_k,\mathbf{Y}_k) &\propto& |\boldsymbol\Sigma_k|^{-\frac{n_k + \nu_{0k} + (p+1) + k + 1}{2}} \\
   & \times& \exp \left \{ -\frac{1}{2} \text{tr} \left (\boldsymbol\Sigma_k^{-1} [\mathbf{V}_{0k} + (\mathbf{Y}_k - \mathbf{X}^*_k \mathbf{B}^*_k)^T (\mathbf{Y}_k - \mathbf{X}^*_k \mathbf{B}^*_k) + (\mathbf{B}^*_k - \mathbf{B}^*_{0k})^T \mathbf{L}_{0k}^{-1} (\mathbf{B}^*_k - \mathbf{B}^*_{0k})] \right )\right \}
\end{eqnarray*}	
\normalsize
Thus, $\boldsymbol\Sigma_k|\mathbf{B}^*_k,\mathbf{Y}_k \sim \text{IW}(\nu_k,\mathbf{V}_k)$, where
\begin{eqnarray*}
    \nu_k&=&\nu_0+n_k+p+1,~~\text{and}\\
	\mathbf{V}_k &=&  \mathbf{V}_{0k}+ (\mathbf{B}_k^* - \mathbf{B}^*_{0k})^T \mathbf{L}^{-1}_{0k}(\mathbf{B}^*_k - \mathbf{B}^*_{0k}) + (\mathbf{Y}_k-\mathbf{X}^*_k\mathbf{B}^*_k)^T (\mathbf{Y}_k - \mathbf{X}^*_k\mathbf{B}^*_k),
\end{eqnarray*}	
as outlined in Proposition 1 of the manuscript.

\section*{Web Appendix B: MCMC Algorithm}

In this section we outline Gibbs updates of all model parameters. For ease of notation, each parameter update is implicitly assumed to be conditional on the data and other model parameters. All notation is defined as in Section 3. The algorithm presented below is not necessarily optimized for computational efficiency.

\begin{enumerate}
	\item \textit{Update missing responses}, $\mathbf{y}_i^{miss}$. For $i = 1,...,n$ and given $z_i=k$:
	\begin{enumerate}
		\item Compute $\boldsymbol\mu^{cond}_{ki}= \boldsymbol\mu^{miss}_{ki} + \boldsymbol\Sigma_{k12} \boldsymbol\Sigma_{k22}^{-1}(\mathbf{y}^{obs}_i - \boldsymbol\mu^{obs}_{ki})$ as in equation (18) of the manuscript.
		\item Compute $\boldsymbol\Sigma^{cond}_k= \boldsymbol\Sigma_{k11} - \boldsymbol\Sigma_{k12}\boldsymbol\Sigma_{k22}^{-1}\boldsymbol\Sigma_{k21}$ as in equation (18).
		\item Sample $\mathbf{y}^{miss}_i$ from $ \text{N}_{J-q_i}(\boldsymbol\mu^{cond}_{ki},\boldsymbol\Sigma^{cond}_k)$ as in equation (18).
	\end{enumerate}
	\item  \textit{For $k = 1,...,K$, use P\'olya-Gamma data augmentation to update the logistic regression parameters, $\boldsymbol\gamma_k, \mathbf{b}_k=(b_{ki},\ldots,b_{kn_k})^T$, and $\sigma^2_k$ for the missing data model described in equation (19) of Section 3.4}.
	\begin{enumerate}
		\item Compute $n_k = \sum_{i = 1}^n \mathds{1}_{(z_i = k)}$, the number of subjects in cluster $k$.
		\item For $i = 1,...,n_k$ and $j = 1,...,J$:
		\begin{enumerate}
			\item Compute $\text{logit}(\phi_{kij})=\mathbf{x}^T_{ij}\boldsymbol\gamma_k+b_{ki}$ as in equation (19), where $\mathbf{x}_{ij}$ is an $m \times 1$ vector of covariates that may overlap with those used in the MSN model, and $\boldsymbol\gamma_k$ is an $m \times 1$ vector of regression parameters as in equation (19).
			\item Update P\'olya-Gamma weights $w_{kij}$ from $\text{PG}[1,\text{logit}(\phi_{kij})]$.
			\item Compute $h_{kij} = \frac{R_{kij} - 1/2}{w_{kij}} - b_{ki}$, where $R_{kij}$ is the binary indicator of whether the response for subject $i$ in cluster $k$ at timepoint $j$ is missing, as in equation (17) of the manuscript, and $b_{ki}$ is the random intercept for subject $i$ in cluster $k$, as in equation (19).
		\end{enumerate}
		\item Form the vector $\mathbf{h}_k = (h_{k11},...,h_{kn_kJ} )^T$.
		\item Form the matrix $\mathbf{O}_k = \text{diag}(w_{k11},...,w_{kn_kJ})$.
		\item Compute $\mathbf{G}_k = (\mathbf{G}_{0k}^{-1} + \mathbf{X}_k^T \mathbf{O}_k \mathbf{X}_k)^{-1}$, where $\mathbf{G}_{0k}$ is the $m \times m$ prior covariance of $\boldsymbol\gamma_k$.
		\item Compute $\mathbf{g}_k = \mathbf{G}_k(\mathbf{G}_{0k}^{-1}\mathbf{g}_{0k} + \mathbf{X}^T_k \mathbf{O}_k \mathbf{h}_k)$, where $\mathbf{g}_{0k}$ is the prior mean of $\boldsymbol\gamma_k$.
		\item Compute $\tau_k = 1/\sigma^2_k$, where $\sigma^2_k$ is the variance of $b_{ki}$.
		\item For $i = 1,...,n_k$:
		\begin{enumerate}
			\item Compute $v_{ki} = (\tau_k + \sum_{j = 1}^Jw_{kij})^{-1}$
			\item Compute $m_{ki} = v_{ki}(\sum_{j = 1}^J w_{kij} (h_{kij} - \mathbf{x}_{ij}^T \boldsymbol\gamma_k))$.
			\item Update $b_{ki}$ from $\text{N}(m_{ki},v_{ki})$.
		\end{enumerate}
		\item Update $\sigma^2_k$ from $\text{IG}(\lambda_{1k} + n_k/2, \lambda_{2k} + (\sum_{i = 1}^{n_k}b_{ki}^2)/2)$, where $\sigma^2_k$ is assumed to have a $\text{IG}(\lambda_{1k},\lambda_{2k})$ prior distribution. Alternatively, update $\tau_k$ from a $\text{Gamma}(\lambda_{1k} + n_k/2, \lambda_{2k} + (\sum_{i = 1}^{n_k}b_{ki}^2)/2)$, where $\text{Gamma}(a,b)$ denotes a gamma distribution with shape parameter $a$ and rate parameter $b$.
	\end{enumerate}
	\item \textit{Update the multinomial logit regression parameters for the cluster allocation model as described in Section 3.2.} For $k = 1,...,K-1$:
	\begin{enumerate}
		\item For $i = 1,...n$:
		\begin{enumerate}
			\item Define $U_{ki} = \mathds{1}_{(z_i = k)}$ as in equation (12).
			\item Compute $c_{ki} = \log (1 + \sum_{h \notin \{k,K \}}e^{\mathbf{w}_i^T \boldsymbol\delta_h})$ as described in Section 3.2.
			\item Compute $\eta_{ki} = \mathbf{w}_i^T \boldsymbol\delta_{k} - c_{ki}$ as in equation (13).
			\item Update $\omega_{ki}$ from $\text{PG}(1,\eta_{ki})$.
		\end{enumerate}	
		\item Define $\mathbf{U}^*_{k} = \left( \frac{U_{k1}-1/2}{\omega_{k1}} + c_{k1},...,\frac{U_{kn}-1/2}{\omega_{kn}} + c_{kn} \right )^T$ as described in Section 3.2.
		\item Compute $\mathbf{S}_{k} = (\mathbf{S}^{-1}_{0k} + \mathbf{W}^T\mathbf{O}_k\mathbf{W})^{-1}$, where $\mathbf{O}_k = \text{diag}(\omega_{k1},...,\omega_{kn})$ and $\mathbf{S}_{0k}$ is the prior covariance of $\boldsymbol\delta_k$.
		\item Compute $\mathbf{d}_{k} = \mathbf{S}_{k}(\mathbf{S}^{-1}_{0k}\mathbf{d}_{0k} + \mathbf{W}^T \mathbf{O}_k \mathbf{U}^*_k)$, where $\mathbf{d}_{0k}$ is the prior mean of $\boldsymbol\delta_k$.
		\item Update $\boldsymbol\delta_k$ from $\text{N}_r(\mathbf{d}_{k},\mathbf{S}_{k})$.
	\end{enumerate}
	\item \textit{Update cluster indicators} $z_1,...,z_n$. For $i = 1,...,n$, iterate through the following steps:
	\begin{enumerate}
		\item For $k = 1,...,K$:
		\begin{enumerate}
			\item Compute $p_{ki} = \text{dnorm}(\mathbf{y}_i;\boldsymbol\mu_{ki},\boldsymbol\Sigma_k)$, where $\text{dnorm}(\mathbf{y};\boldsymbol\mu,\boldsymbol\Sigma)$ denotes the density of a multivariate normal
            random variable with mean $\boldsymbol\mu$ and covariance $\boldsymbol\Sigma$ evaluated at $\mathbf{y}$; and $\boldsymbol\mu_{ki} = \mathbf{X}_i \boldsymbol\beta_k + t_i\boldsymbol\psi_k$, where $\mathbf{X}_i$ is the $J \times Jp$ design matrix defined in equation (3) and \break $\boldsymbol{\beta}_k=\text{vec}(\mathbf{B}_k) = (\beta_{k11},\ldots,\beta_{k1p},\ldots,\beta_{kJ1},\ldots,\beta_{kJp})^T$ is a $Jp\times 1$ vector of cluster- and outcome-specific regression coefficients also defined as in equation (3). When covariates are not time dependent, we may simplify notation to $\boldsymbol\mu_{ki} = \mathbf{B}^{*T}_k \mathbf{x}^{*}_{ki}$, where $\mathbf{x}^{*T}_{ki}$ is given by the $i^{th}$ row of the $n_k \times (p+1)$ matrix $\mathbf{X}^*_k$, where
\[
\underset{n_k\times(p+1)}{\mathbf{X}^*_k}=\begin{pmatrix}x_{11} & \ldots & x_{1p} & t_{ki}\\
              \vdots & \ddots & \vdots & \vdots\\
              x_{n_k1} & \ldots & x_{n_kp} & t_{kn_k}\end{pmatrix}
 ~~\text{and}~~
\underset{(p+1)\times J}{\mathbf{B}^*_k}=\begin{pmatrix} \beta_{k11} & \ldots & \beta_{kJ1}\\
              \vdots & \ddots & \vdots \\
              \beta_{k1p} & \ldots & \beta_{kJp} \\
		  \psi_{k1} & \ldots & \psi_{kJ}\end{pmatrix},\vspace{6pt}
\]
as in Section 3.1.
			\item From equation (19) in the manuscript, compute $\phi_{ki} = \text{logit}^{-1}(\mathbf{x}_{ij}^T \boldsymbol\gamma_k + b_i)$.
			\item Compute $\rho_{ki} = \prod_{j = 1}^J \text{dbern}(R_{ij};\phi_{ki})$, where $\text{dbern()}$ denotes the Bernoulli distribution function.
		\end{enumerate}
		\item Compute $\boldsymbol\pi_i = (\pi_{1i},...,\pi_{Ki})$, where $\pi_{ki} = \frac{e^{\mathbf{w}_i^T \boldsymbol\delta_k}}{\sum_{h = 1}^K e^{\mathbf{w}_i^T \boldsymbol\delta_h}}$ for $k = 1,...,K$, as denoted in equation (8) of the manuscript. Recall that cluster $K$ serves as the reference category, implying that $\boldsymbol\delta_K=\boldsymbol0$.
		\item Compute the posterior probability $\text{Pr}(z_i = k) = \frac{\pi_{ki}p_{ki}\rho_{ki}}{\sum_{l = 1}^K \pi_{li}p_{li}\rho_{li}}$, for $k = 1,...,K$. Note that under (marginal) MAR imputation, $\rho$ is left out of this equation, as the missing data model is fully ignorable in this case.
		\item Update $z_i$ from $\text{Categorical}[\text{Pr}(z_i = 1),...,\text{Pr}(z_i = K)]$.
	\end{enumerate}
	
\item \textit{Update the multivariate skew normal regression parameters as described in Section 3.1.} We first consider the case where there are no time-dependent covariates. We then consider time-varying designs.
     \begin{enumerate}
      \item Time-Invariant Designs:
      \begin{enumerate}
      \item For $i=1,\ldots,n$ and given $z_i=k$, update $t_i$ from its $\text{N}_{+}(a_{ki},A_{k})$ full conditional, where $\text{N}_{+}()$ denotes a truncated normal random variable restricted to the positive real line,
          \begin{eqnarray*}
          A_k&=&(1 + \boldsymbol\psi_k^T \boldsymbol\Sigma_k^{-1} \boldsymbol\psi_k)^{-1},\\
          a_{ki}&=&A_k \boldsymbol\psi_k^T \boldsymbol\Sigma_k^{-1} (\mathbf{y}_i - \mathbf{B}_k^T\mathbf{x}_{ki}),
          \end{eqnarray*}
          $\mathbf{y}_{i}=(y_{i1},\ldots,y_{iJ})^T$, $\boldsymbol\psi_k=(\psi_{k1},\ldots,\psi_{kJ})^T$, $\mathbf{B}_k$ is the $p \times J$ matrix defined in equation (5) of the manuscript, and $\mathbf{x}_{ki}$ is the $p\times 1$ vector formed from the $i$-th row of $\mathbf{X}_k$ from equation (5).\vspace{2pt}
      \item  For $k=1,\ldots,K$, draw $\mathbf{B}^*_k$ from $\text{MatNorm}_{(p+1)\times J}(\mathbb{B}^*_k,\mathbf{L}_k,\boldsymbol\Sigma_k)$ as described in Proposition 1.  Note that the $(p+1)^{th}$ row of $\mathbf{B}^*_k$ contains $\boldsymbol\psi_k=(\psi_{k1},\ldots,\psi_{kJ})^T$. Therefore, we vectorize $\mathbf{B}^*_k$ into $Jp \times 1$ vector $\boldsymbol\beta_k$ and $J\times 1$ vector $\boldsymbol\psi_k$ to perform the back transformations described in equation (4) of the manuscript. To draw from the matrix normal density, make use of the R package \texttt{matrixsample} (Laurent, 2018). \vspace{2pt}
      \item   For $k=1,\ldots,K$, update $\underset{J\times J}{\boldsymbol\Sigma_k}$ from $\text{IW}(\nu_k,\mathbf{V}_k)$ as described in Proposition 1.
     \end{enumerate}

      \item Time-Varying Designs: For designs that include time-varying covariates, we work with equation (3) in the manuscript.
      \begin{enumerate}
           \item For $i=1,\ldots,n$ and given $z_i=k$, update $t_i$: To update $t_i$ given $z_i=k$, we create a $J \times Jp$ design matrix $\mathbf{X}_i$ and $Jp \times 1$ vector $\boldsymbol\beta_k$ of the form
               \begin{eqnarray*}
               \underset{J\times Jp}{\mathbf{X}_i}&=&\begin{pmatrix}x_{i11} & \ldots & x_{i1p}     & 0       & \ldots           &  0       & \ldots  & 0          \\
                                                                                  &        & \vdots      & \ddots  & \vdots           &          &         &             \\
                                                                          0       & \ldots & 0           & 0       &  \ldots          &  x_{iJ1} & \ldots  & x_{iJp}
                              \end{pmatrix}\\[8pt]
               \underset{Jp \times 1}{\boldsymbol\beta_k}&=&(\beta_{k11},\ldots,\beta_{k1p},\ldots,\beta_{kJ1},\ldots,\beta_{kJp})^T.
               \end{eqnarray*}
          Next, we draw $t_i|(z_i=k)$ from its $\text{N}_{+}(a_{ki},A_{k})$ full conditional, where
          \begin{eqnarray*}
          A_k&=&(1 + \boldsymbol\psi_k^T \boldsymbol\Sigma_k^{-1} \boldsymbol\psi_k)^{-1},\\
          a_{ki}&=&A_k \boldsymbol\psi_k^T \boldsymbol\Sigma_k^{-1} (\mathbf{y}_i - \mathbf{X}_i\bfb_k),
          \end{eqnarray*}
          $\mathbf{y}_{i}=(y_{i1},\ldots,y_{iJ})^T$, and $\boldsymbol\psi_k=(\psi_{k1},\ldots,\psi_{kJ})^T$.\vspace{2pt}
          \item For $k=1,\ldots,K$, update $\boldsymbol\beta_k$, $\boldsymbol\psi_k$: To update the regression parameters, we create an augmented $J \times J(p+1)$ design matrix $\mathbf{X}^*_i$ and $J(p+1) \times 1$ vector $\boldsymbol\beta^*_k$ of the form
               \begin{eqnarray*}
               \underset{J\times J(p+1)}{\mathbf{X}^*_i}&=&\begin{pmatrix}x_{i11} & \ldots & x_{i1p}  & t_i    & 0       & \ldots           &  0       & \ldots  & 0       & 0    \\
                                                     &        &          & \vdots & \ddots  & \vdots           &          &         &         &       \\
                                             0       & \ldots & 0        & 0      &  0      &  \ldots          &  x_{iJ1} & \ldots  & x_{iJp} & t_i
                              \end{pmatrix}\\[8pt]
               \underset{J(p+1) \times 1}{\boldsymbol\beta^*_k}&=&(\beta_{k11},\ldots,\beta_{k1p},\psi_1,\ldots,\beta_{kJ1},\ldots,\beta_{kJp},\psi_J)^T.
               \end{eqnarray*}

           Next, for all $k$, we assign independent multivariate normal and IW priors to $\boldsymbol\beta^*_k$ and $\boldsymbol\Sigma_k$:
           \begin{eqnarray*}
           \underset{J(p+1) \times 1}{\boldsymbol\beta^*_k}&\sim& \text{N}_{J(p+1)}\left(\boldsymbol\beta_0,\mathbf{T}_0^{-1}\right)~~\text{and}\\
          \underset{J\times J}{\boldsymbol\Sigma_k}&\sim& \text{IW}(\nu_0,\mathbf{S}_0),
           \end{eqnarray*}
           where $\mathbf{T}_0$ is a $J(p+1)\times J(p+1)$ prior precision matrix and, in this context, $\mathbf{S}_0$ is a $J\times J$ prior scale matrix. Following standard algebraic routines for conjugate multivariate normal priors, we arrive at the following full conditional for $\boldsymbol\beta^*_k$:
           \begin{eqnarray*}
           \underset{J(p+1) \times 1}{\boldsymbol\beta^*_k}&\sim& \text{N}_{J(p+1)}\left(\mathbf{m}_k,\mathbf{V}_k\right),~~\text{where}\\
           \underset{J(p+1)\times J(p+1)}{\mathbf{V}_k}&=&\left[\mathbf{T}_0+\mathbf{X}^{*T}_k\left(\mathbf{I}_{n_k}\otimes\boldsymbol\Sigma^{-1}_k\right)\mathbf{X}^*_k\right]^{-1} ~~\text{and}\\
           \underset{J(p+1)\times 1}{\mathbf{m}_k}&=&\mathbf{V}_k\left[\mathbf{T}_0\boldsymbol\beta_0 + \mathbf{X}^{*T}_k\left(\mathbf{I}_{n_k}\otimes\boldsymbol\Sigma^{-1}_k\right)\mathbf{y}_k\right].
            \end{eqnarray*}
           Here, $\mathbf{y}_k$ denotes the $Jn_k\times 1$ vector of responses for each observation in cluster $k$ after imputation, and here $\mathbf{X}^*_k$ denotes an $Jn_k \times J(p+1)$ matrix formed by stacking $\mathbf{X}^*_i$ for all subjects in cluster $k$. To perform the back transformations described in equation (4) of the manuscript, we extract the $Jp\times 1$ vector $\boldsymbol\beta_k$ and the $J\times 1 $ vector $\boldsymbol\psi_k=(\psi_{k1},\ldots,\psi_{kJ})^T$ from $\boldsymbol\beta^*_k$.\vspace{4pt}

          \item Finally, for $k=1,\ldots,K$, we update $\boldsymbol\Sigma_k$ from IW$(\nu_k,\mathbf{S}_k)$ where
               \begin{eqnarray*}
                \nu_k&=&\nu_0+n_k ~~\text{and}\\
                \underset{J\times J}{\mathbf{S}_k}&=&\mathbf{S}_0+\mathbf{R}_k^T\mathbf{R}_k,
                \end{eqnarray*}
           where $\mathbf{R}_k$ is an $n_k \times J$ matrix with $i$-th row equal to $(\mathbf{y}_i-\mathbf{X}^*_i\boldsymbol\beta^*_k)^T$ for all $i$ in cluster $k$.
          \end{enumerate}

     \item  For both time-varying and time-invariant designs, we back transform to obtain $\boldsymbol\alpha_k$ and $\boldsymbol\Omega_k$ as described in equation (4) of the manuscript. In the time-invariant case, we
            vectorize the $(p+1)\times J$ matrix $\mathbf{B}^*_k$ into $Jp \times 1$ vector $\boldsymbol\beta_k$ and $J\times 1$ vector $\boldsymbol\psi_k$ prior to back transforming. In the time-varying setting, we extract the $Jp\times 1$ vector $\boldsymbol\beta_k$ and the $J\times 1 $ vector $\boldsymbol\psi_k=(\psi_{k1},\ldots,\psi_{kJ})^T$ from $\boldsymbol\beta^*_k$, and use these to perform the back transformations.
   \end{enumerate}
	\item (Optional) \textit{Update latent scaling terms for extension to skew-t model as described in Section 3.3}.
	\begin{enumerate}
        \item Time invariant designs:
        \begin{enumerate}
		\item Compute $s_1 = \frac{\xi + J + 1}{2}$, where $\xi$ is the pre-specified degrees of freedom parameter.\vspace{2pt}
		\item For $i = 1,...,n$, compute $s_{2i} = \frac{\xi + t_i^2 + (\mathbf{y}_i - \mathbf{B}^{*T}_k\mathbf{x}_{ki}^* )^T\boldsymbol\Sigma_k^{-1}(\mathbf{y}_i -  \mathbf{B}^{*T}_k\mathbf{x}_{ki}^* )}{2}$, where
              $\mathbf{B}^*_k$ is the $(p+1)\times J$ parameter matrix defined in equation (7) of the manuscript and $\mathbf{x}_{ki}^*$ is a $(p+1)\times 1$ vector formed from the $i$-th row of $\mathbf{X}^*_k$ in (7).\vspace{2pt}
		\item For $i = 1,...,n$, update $d_{i}$ from $\text{Gamma}(s_1,s_{2i})$.\vspace{2pt}
		\item For $k=1,\ldots,K$, form the $n_k\times J$ scaled matrix $\tilde{\mathbf{Y}}_{k}=\sqrt{\mathbf{d}_k}\circ\mathbf{Y}_{k}$, where ``$\circ$'' denotes the Hadamard product. Use $\tilde{\mathbf{Y}}_{k}$ in place of
              $\mathbf{Y}_k$ for
              all remaining updates.\vspace{2pt}
		\item For $k=1,\ldots,K$, form the $n_k \times (p+1)$ scaled matrix $\tilde{\mathbf{X}}_k=\sqrt{\mathbf{d}_k}\circ\mathbf{X}^*_k$. Use $\tilde{\mathbf{X}}_{k}$ in place of $\mathbf{X}_k$ for all remaining
              updates.\vspace{2pt}
        \end{enumerate}

        \item Time-varying designs
         \begin{enumerate}
		\item Compute $s_1 = \frac{\xi + J + 1}{2}$, where $\xi$ is the pre-specified degrees of freedom parameter.\vspace{2pt}
		\item For $i = 1,...,n$, compute $s_{2i} = \frac{\xi + t_i^2 + (\mathbf{y}_i - \mathbf{X}^*_i\boldsymbol\beta^*_k)^T\boldsymbol\Sigma_k^{-1}(\mathbf{y}_i -  \mathbf{X}^*_i\boldsymbol\beta^*_k)}{2}$, where
              $\mathbf{X}^*_i$ is the $J\times J(p+1)$  matrix and $\boldsymbol\beta^*_k$ is the $J(p+1)$ parameter vector each defined in Step 5(b) above.\vspace{2pt}
		\item For $i = 1,...,n$, update $d_{i}$ from $\text{Gamma}(s_1,s_{2i})$.\vspace{2pt}
        \item Use $d_i$ to scale the $J\times 1$ response vector $\mathbf{y}_i$ and the $J \times J(p+1)$ matrix $\mathbf{X}^*_i$ from Step 5(b). Next, for $k=1,\ldots,K$, combine data for all subjects in cluster $k$ to form
              the $Jn_k\times 1$ scaled vector $\tilde{\mathbf{y}}_k$ and $Jn_k\times J(p+1)$ scaled matrix $\tilde{\mathbf{X}}^*_k$. Use the scaled data for all remaining updates.
        \end{enumerate}
	\end{enumerate}
\end{enumerate}
\clearpage
\section*{Web Appendix C: Web Tables}

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
%\begin{comment}
\begin{table}[!htb]
\caption{\label{tab:t1} Sample characteristics from the Nurture study $(n=560,\,N=1769)$.}
\begin{center}
\begin{tabular}{@{}ll@{}}
\toprule
& \underline{$n$ (\%)}\\
No. Missing Observations$^{\dagger}$ & 471 (21.0)\\
Missing 3 mo. & 113 (20.2) \\
Missing 6 mo. & 112 (20.00) \\
Missing 9 mo. & 131 (23.4) \\
Missing 12 mo. & 115 (20.5) \\
Food Insecure & 216 (38.6) \\
Infant Gender (Female)  & 277 (49.5) \\
Infant Race (Black)  & 378 (67.5) \\
Any Breastfeeding & 213 (38.0) \\[6pt]
& \underline{Median (IQR)}\\
Bayley composite score at 3 mo. & 110.0 (15.0) \\
Bayley composite score at 6 mo. & 103.0 (18.0) \\
Bayley composite score at 9 mo. & 100.0 (19.0) \\
Bayley composite score at 12 mo. & 97.0 (16.0) \\[6pt]
& \underline{Mean (SD)}\\
Birth weight for gestational age z-score & 0.1 (1.0) \\
Total number of children in household & 2.5 (1.5) \\ \bottomrule
\end{tabular}
\end{center}\vspace{-1em}
\hspace{1.85in}$^{\dagger}$ Number missing out of $560\times 4=2240$ possible observations.
\end{table}
%\end{comment}

\begin{table}[!htb]
\centering
\caption{\label{tab:cor_mat} Estimated correlation matrix from repeated measures model with unstructured correlation.}
\begin{tabular}{l|llll}
 & 3 mo. & 6 mo. & 9 mo. & 12 mo. \\ \hline
3 mo. & 1.00  &  &  &  \\
6 mo. & 0.23 &  1.00 &  &  \\
9 mo. & 0.15 & 0.24 &  1.00 &  \\
12 mo. & 0.15 & 0.25 & 0.27 & 1.00
\end{tabular}
\end{table}

\newpage

\begin{landscape}\begin{table}[t]

\caption{\label{tab:sim1}Model results for Simulation 1: Multivariate skew normal (MSN) and multivariate normal (MVN) models fit to MSN data with varying skewness settings, $n = 1000$, $J = 4$, $p = 2$, $K = 3$, and $r = 2$. Cluster 3 corresponds to data generated under a MVN model ($\boldsymbol\alpha_3=\boldsymbol0$). 10000 iterations were run with a burn-in of 1000.}
\begin{center}
\fontsize{7}{9}\selectfont
\begin{tabular}{lllllllllll}
\toprule
\multicolumn{2}{c}{ } & \multicolumn{3}{c}{Cluster 1} & \multicolumn{3}{c}{Cluster 2} & \multicolumn{3}{c}{Cluster 3} \\
\cmidrule(l{3pt}r{3pt}){3-5} \cmidrule(l{3pt}r{3pt}){6-8} \cmidrule(l{3pt}r{3pt}){9-11}
Component & Param. & True & MSN Est. (95\% CrI) & MVN Est. (95\% CrI)  & True & MSN Est. (95\% CrI) & MVN Est. (95\% CrI) & True & MSN Est. (95\% CrI) & MVN Est. (95\% CrI)\\
\midrule
\addlinespace[0.3em]
\multicolumn{11}{l}{\textbf{ }}\\
\hspace{1em}MVSN & $\beta_{k11}$ & 110.00 & 110.20 (109.97, 110.41) & 106.36 (105.97, 108.71) & 90.00 & 90.17 (89.85, 90.44) & 88.43 (88.05, 88.81) & 100.00 & 100.02 (99.68, 100.70) & 100.02 (99.82, 100.23)\\
\hspace{1em}Regression & $\beta_{k21}$ & 115.00 & 115.13 (114.91, 115.33) & 104.17 (103.93, 104.44) & 85.00 & 85.31 (85.00, 85.58) & 83.00 (82.57, 83.46) & 100.00 & 100.25 (99.52, 100.73) & 99.99 (99.79, 100.18)\\
\hspace{1em} & $\beta_{k31}$ & 120.00 & 120.08 (119.83, 120.49) & 128.02 (128.57, 129.08) & 80.00 & 80.23 (79.89, 80.54) & 70.59 (70.08, 71.10) & 100.00 & 100.13 (99.48, 100.77) & 100.04 (99.83, 100.26)\\
\hspace{1em} & $\beta_{k41}$ & 125.00 & 125.15 (124.86, 125.49) & 126.67 (126.31, 127.05) & 75.00 & 74.94 (74.61, 75.26) & 72.19 (71.64, 72.72) & 100.00 & 99.81 (99.24, 100.40) & 99.99 (99.78, 100.21)\\
\hspace{1em} & $\beta_{k12}$ & 1.00 & 0.97 (0.84, 1.11) & 0.90 (0.74, 1.08) & -1.00 & -1.08 (-1.25, -0.92) & -1.12 (-1.29, -0.93) & -1.00 & -1.00 (-1.10, -0.89) & -1.00 (-1.10, -0.89)\\
\hspace{1em} & $\beta_{k22}$ & 1.50 & 1.51 (1.40, 1.62) & 1.53 (1.41, 1.66) & -1.50 & -1.51 (-1.73, -1.33) & -1.66 (-1.77, -1.52) & 1.00 & 0.99 (0.89, 1.08) & 0.98 (0.89, 1.08)\\
\hspace{1em} & $\beta_{k32}$ & 2.00 & 2.01 (1.89, 2.14) & 2.20 (2.08, 2.33) & -2.00 & -1.99 (-2.22, -1.78) & -2.44 (-2.67, -2.17) & -1.00 & -0.92 (-1.01, -0.82) & -0.91 (-1.01, -0.81)\\
\hspace{1em} & $\beta_{k42}$ & 2.50 & 2.50 (2.35, 2.66) & 2.46 (2.28, 2.64) & -2.50 & -2.52 (-2.77, -2.29) & -2.68 (-2.92, -2.39) & 1.00 & 1.04 (0.94, 1.15) & 1.05 (0.95, 1.15)\\
\addlinespace[0.3em]
\multicolumn{11}{l}{\textbf{ }}\\
\hspace{1em} & $\Sigma_{k11}$ & 1.00 & 0.96 (0.77, 1.14) & 2.42 (2.06, 2.84)& 1.00 & 0.96 (0.69, 1.48) & 2.53 (2.17, 3.01) & 1.00 & 0.98 (0.80, 1.14) & 1.01 (0.88, 1.15)\\
\hspace{1em} & $\Sigma_{k12}$ & 0.50 & 0.47 (0.34, 0.61) & 1.20 (0.99, 1.48) & 0.50 & 0.50 (0.27, 0.99) & 2.51 (2.14, 3.04) & 0.50 & 0.51 (0.21, 0.71) & 0.51 (0.41, 0.61)\\
\hspace{1em} & $\Sigma_{k13}$ & 0.25 & 0.25 (0.04, 0.40) & -0.54 (-0.75, -0.34) & 0.25 & 0.26 (0.13, 0.72) & 2.62 (2.20, 3.17) & 0.25 & 0.25 (0.14, 0.37) & 0.25 (0.15, 0.36)\\
\hspace{1em} & $\Sigma_{k14}$ & 0.12 & 0.11 (-0.02, 0.30) & -1.35 (-1.67, -1.06) & 0.12 & 0.15 (-0.06, 0.67) & 2.72 (2.24, 3.29) & 0.12 & 0.09 (-0.12, 0.29) & 0.09 (-0.01, 0.20)\\
\hspace{1em} & $\Sigma_{k22}$ & 1.00 & 0.99 (0.74, 1.19) & 1.20 (0.99, 1.48) & 1.00 & 1.03 (0.78, 1.54) & 2.51 (2.14, 3.04) & 1.00 & 0.99 (0.71, 1.24)& 0.91 (0.80, 1.05)\\
\hspace{1em} & $\Sigma_{k23}$ & 0.50 & 0.49 (0.26, 0.66) & 1.24 (1.06, 1.46) & 0.50 & 0.59 (0.38, 1.03) & 3.69 (3.18, 4.35) & 0.50 & 0.56 (0.37, 0.71) & 0.44 (0.34, 0.54)\\
\hspace{1em} & $\Sigma_{k24}$ & 0.25 & 0.24 (0.10, 0.43) & 0.08 (-0.06, 0.21) & 0.25 & 0.28 (-0.01, 0.61) & 3.65 (3.09, 4.31) & 0.25 & 0.25 (0.14, 0.37) & 0.27 (0.17, 0.37)\\
\hspace{1em} & $\Sigma_{k33}$ & 1.00 & 0.99 (0.77, 1.09) & 1.24 (1.06, 1.46) & 1.00 & 1.09 (0.88, 1.59) & 3.65 (3.03, 4.32) & 1.00 & 0.95 (0.80, 1.10) & 1.00 (0.87, 1.15)\\
\hspace{1em} & $\Sigma_{k34}$ & 0.50 & 0.47 (0.22, 0.65) & 1.15 (0.93, 1.40) & 0.50 & 0.54 (0.25, 0.99) & 2.62 (2.20, 3.17) & 0.50 & 0.57 (0.39, 0.73) & 0.56 (0.45, 0.70)\\
\hspace{1em} & $\Sigma_{k44}$ & 1.00 & 1.01 (0.63, 1.23) & 2.48 (2.15, 2.91) & 1.00 & 1.02 (0.64, 1.60) & 3.65 (3.09, 4.31) & 1.00 & 1.06 (0.81, 1.60) & 1.06 (0.94, 1.23)\\
\addlinespace[0.3em]
\multicolumn{11}{l}{\textbf{ }}\\
\hspace{1em} & $\alpha_{k1}$ & -2.00 & -2.05 (-2.28, -1.66) & / & -2.00 & -2.19 (-2.50, -1.77) & / & 0.00 & -0.23 (-0.80, 0.42) & /\\
\hspace{1em} & $\alpha_{k2}$ & -1.00 & -1.01 (-1.30, -0.75) & / & -2.50 & -2.52 (-2.82, -2.10) & / & 0.00 & -0.33 (-0.94, 0.57)& /\\
\hspace{1em} & $\alpha_{k3}$ & 1.00 & 0.97 (0.65, 1.28) & / & -3.00 & -3.34 (-3.67, -2.90) & / & 0.00 & -0.12 (-0.93, 0.68) & /\\
\hspace{1em} & $\alpha_{k4}$ & 2.00 & 1.97 (1.67, 2.28) & / & -3.50 & -3.49 (-3.84, -3.00) & / & 0.00 & 0.23 (-0.51, 0.94) & /\\
\addlinespace[0.3em]
\multicolumn{11}{l}{\textbf{ }}\\
\hspace{1em}Multinomial & $\delta_{k1}$ & -0.27 & -0.23 (-0.47, -0.09) & -0.14 (-0.35, 0.08) & 0.14 & 0.12 (0.01, 0.21) & 0.20 (0.00, 0.42) & Ref. &  Ref. &  Ref.\\
\hspace{1em}Logit$^{\dagger}$ &$\delta_{k2}$ & 0.07 & 0.07 (-0.26, 0.39) & 0.08 (-0.24, 0.38) & 0.17 & 0.16 (0.01, 0.38) & 0.02 (-0.28, 0.30) & Ref. &  Ref. &  Ref.\\
\addlinespace[0.3em]
\multicolumn{11}{l}{\textbf{ }}\\
\hspace{1em}Missing & $\gamma_{k1}$ & -0.82 & -0.84 (-0.96, -0.73) & -1.08 (-1.19, -0.99) & -0.93 & -0.93 (-1.05, -0.81) & -1.02 (-1.15, -0.93) & -1.19 & -1.22 (-1.39, -1.10) & -0.78 (-0.91, -0.67)\\
\hspace{1em}Data& $\gamma_{k2}$ & -1.08 & -1.01 (-1.20, -0.91) & -1.80 (-1.96, -1.64) & -1.14 & -1.11 (-1.25, -1.00) & -0.72 (-0.80, -0.59) & -0.97 & -0.93 (-1.10, -0.79) & -1.09 (-1.22, -0.98)\\
\hspace{1em} & $\gamma_{k3}$ & -1.12 & -1.08 (-1.20, -1.00) & -0.90 (-1.00, -0.80) & -0.98 & -0.99 (-1.12, -0.85) & -1.04 (-1.16, -0.90) & -0.87 & -0.88 (-1.02, -0.76) & -0.97 (-1.09, -0.86)\\
\hspace{1em} & $\sigma^2_{k}$ & 1.00 & 1.07 (0.92, 1.28) & 0.89 (0.76, 1.07) & 1.00 & 0.96 (0.83, 1.11) & 1.21 (1.05, 1.41) & 1.00 & 1.11 (0.96, 1.30) & 0.91 (0.80, 1.05)\\
\addlinespace[0.3em]
\multicolumn{11}{l}{\textbf{ }}\\
\hspace{1em}Clustering$^{\ddagger}$ & $\pi_l$ & 0.32 & 0.32 (0.31, 0.33) & 0.32 (0.30, 0.34) & 0.29 & 0.29 (0.28, 0.30) & 0.29 (0.28, 0.30) & 0.39 & 0.39 (0.39, 0.39) & 0.39 (0.39, 0.39)\\
\bottomrule
\end{tabular}\vspace{-.25cm}
\end{center}
$^{\dagger}$ Multinomial logit parameters comparing clusters 1 and 2 to cluster 3 (reference cluster).\\
$^{\ddagger}$ Estimated proportion in each cluster. True proportions are 0.32, 0.29 and 0.39, respectively.
\end{table}
\end{landscape}

\begin{table}[t]
\caption{\label{tab:sim2} Results for clusters 2 and 3 from Simulation 2. Posterior means (95\% CrIs) are presented under online missing not at random (MNAR) imputation, online missing at random (MAR) imputation, and Bayesian multiple imputation (MI) as described in Section 4.2 of the manuscript. 10000 iterations were run with a burn-in of 1000.}
\begin{center}
\begin{tabular}{llllll}
\toprule
\textbf{Model} & & \textbf{True} & & & \\
\textbf{Component} & \textbf{Parameter} & \textbf{Value} & \textbf{MNAR} & \textbf{MAR} & \textbf{MI}\\
\midrule
$\boldsymbol{k = 2}$ & $\beta_{k11}$ & -2.60 & -2.67 (-2.87, -2.46) & -2.54 (-2.87, -2.23) & -3.51 (-3.82, -3.16)\\
 & $\beta_{k21}$ & -0.89 & -0.82 (-1.02, -0.62) & -2.77 (-2.88, -2.67) & -2.26 (-2.36, -2.15)\\
MSN & $\beta_{k31}$ & -1.74 & -1.71 (-1.95, -1.66) & -1.54 (-1.88, -1.25) &-1.88 (-2.93, -1.48)\\
Regression & $\beta_{k41}$ & -2.20 & -1.96 (-2.28, -1.64) & -2.13 (-2.23, -2.03) & -2.67 (-2.77, -2.56)\\
& $\beta_{k12}$ & -2.00 & -2.01 (-3.32, -0.94) & -3.29 (-3.71, -2.89) & -1.07 (-1.65, -0.73)\\
& $\beta_{k22}$ & -2.14 & -1.96 (-2.17, -1.76) & -2.85 (-2.97, -2.74) & -1.35 (-1.44, -1.25)\\
& $\beta_{k32}$ & -2.24 & -2.29 (-3.87, -1.09) & -2.30 (-2.66, -1.99) & -3.31 (-3.71, -2.95)\\
& $\beta_{k42}$ & -2.05 & -1.93 (-2.15, -1.71) & -2.61 (-2.73, -2.51) & -2.48 (-2.58, -2.37)\\
\addlinespace[0.2em]
\multicolumn{5}{l}{\textbf{ }}\\
& $\alpha_{k1}$ & -1.00 & -0.99 (-1.52, -0.15) & -0.72 (-1.04, -0.34) & -0.46 (-0.89, -0.10)\\
& $\alpha_{k2}$ & -1.00 & -0.94 (-1.89, -0.34) & -1.05 (-1.39, -0.65) & -1.02 (-1.52, 0.35)\\
& $\alpha_{k3}$ & -1.00 & -0.96 (-0.85, -0.38) & -0.23 (-0.73, 0.28) & -0.56 (-1.01, 0.15)\\
& $\alpha_{k4}$ & -1.00 & -0.93 (-1.71, -0.37) & -0.50 (-0.86, -0.07) & -0.11 (-0.56, 0.39)\\
\addlinespace[0.2em]
\multicolumn{5}{l}{\textbf{ }}\\
Multinomial & $\delta_{k1}$ & -0.40 & -0.30 (-0.47, -0.12) & 0.06 (-0.13, 0.25) & -0.02 (-0.19, 0.18)\\
Logit$^{\dagger}$& $\delta_{k2}$ & -0.80 & -0.77 (-1.04, -0.5) & -0.31 (-0.6, -0.04) & 0.21 (-0.06, 0.47)\\
\addlinespace[0.2em]
\multicolumn{5}{l}{\textbf{ }}\\
Missing Data & $\gamma_{k1}$ & -0.88 & -0.96 (-1.07, -0.83) &  / & / \\
& $\gamma_{k2}$ & -1.04 & -0.96 (-1.08, -0.83) & / & / \\
& $\gamma_{k3}$ & -0.91 & -0.90 (-1.02, -0.79)& / & / \\
& $\sigma^2_{k}$ & 1.00 & 1.03 (0.90, 1.17) & / & / \\
\midrule
$\boldsymbol{k = 3}$ & $\beta_{k11}$ & 2.20 & 2.08 (1.49, 2.64) & 2.25 (1.85, 2.7) & 1.71 (1.35, 2.09)\\
 & $\beta_{k21}$ & 3.54 & 3.50 (3.28, 3.83) & 1.53 (1.32, 1.74) & 0.95 (0.79, 1.12)\\
MSN & $\beta_{k31}$ & 1.29 & 1.25 (0.64, 2.16) & 1.21 (0.83, 1.58) & 1.60 (1.17, 2.07)\\
Regression & $\beta_{k41}$ & 1.64 & 1.31 (1.09, 1.54) & 0.77 (0.56, 0.98) & 1.80 (1.63, 1.97)\\
& $\beta_{k12}$ & 1.88 & 1.77 (1.11, 2.23) & 2.06 (1.64, 2.60) & 1.64 (1.31, 2.00)\\
& $\beta_{k22}$ & 1.12 & 1.07 (0.82, 1.30) & 1.92 (1.71, 2.14) & 0.50 (0.35, 0.65)\\
& $\beta_{k32}$ & 2.13 & 2.14 (1.38, 3.09) & 2.72 (2.22, 3.25) & 2.96 (2.55, 3.33)\\
& $\beta_{k42}$ & 1.13 & 1.08 (0.67, 1.20) & 2.47 (2.26, 2.69) & 1.95 (1.77, 2.12)\\
\addlinespace[0.2em]
\multicolumn{5}{l}{\textbf{ }}\\
& $\alpha_{k1}$ & 2.00 & 1.88 (1.24, 2.55) & 1.50 (1.02, 1.97) & 1.78 (1.28, 2.13)\\
& $\alpha_{k2}$ & 2.00 & 1.91 (0.68, 2.13) & 2.07 (1.63, 2.50) & 1.76 (1.20, 2.19)\\
& $\alpha_{k3}$ & 2.00 & 1.93 (0.65, 2.12) & 1.70 (0.99, 2.20) & 1.87 (1.48, 2.24)\\
& $\alpha_{k4}$ & 2.00 & 1.92 (1.06, 2.77) & 1.23 (0.62, 1.76) & 1.28 (0.83, 1.73)\\
\addlinespace[0.2em]
\multicolumn{5}{l}{\textbf{ }}\\
Multinomial & $\delta_{k1}$ & Ref. & Ref. & Ref. & Ref.\\
Logit & $\delta_{k2}$ &Ref. & Ref. & Ref. & Ref.\\
\addlinespace[0.2em]
\multicolumn{5}{l}{\textbf{ }}\\
Missing Data & $\gamma_{k1}$ & -0.87 & -0.88 (-1.04, -0.72) &  / & / \\
& $\gamma_{k2}$ & -1.08 & -0.86 (-1.01, -0.71) & / & / \\
& $\gamma_{k3}$ & -0.97 & -0.93 (-1.09, -0.78)& / & / \\
& $\sigma^2_{k}$ & 1.00 & 1.15 (0.97, 1.38) & / & / \\
\bottomrule
\end{tabular}\vspace{-.25cm}
\end{center}
\hspace{.80cm}$^{\dagger}$ Multinomial logit parameters comparing cluster 2 to cluster 3 (reference cluster).
\end{table}

\begin{table}[!htb]
\centering
\caption{\label{tab:sim3} Simulation 3 WAIC values for MSN models fit with $K = 2,3,4,5$ clusters to data simulated from MSN models with $K = 2,3,4,5$. Bold indicates best-fitting model. (*) Model did not converge due to empty or singleton clusters occurring within 10000 iterations of the MCMC algorithm.}
\begin{tabular}{@{}llllll@{}}
\toprule
 &  &  & \textbf{Fitted} &  &  \\ \midrule
 &  & $K = 2$ & $K = 3$ & $K = 4$ & $K = 5$ \\
 & $K = 2$ & \textbf{11624} & 11963  & * & * \\
\textbf{Truth} & $K = 3$ & 15193  & \textbf{12390}  & 12811  & * \\
 & $K = 4$ & 15777  & 14359 & \textbf{12412} & 14237  \\
 & $K = 5$ & 15012  & 14359  & 14323 & \textbf{13436} \\ \bottomrule
\end{tabular}
\end{table}

\begin{table}[!htb]
\centering
\caption{\label{tab:cor_mat} Estimated covariances (95\% CrI), $\boldsymbol\Sigma_1, \boldsymbol\Sigma_2$, from the 2-cluster MSN model fit to the Nurture data as described in Section 5.}
\begin{tabular}{lllll}
\toprule
k = 1 & 3 mo. & 6 mo. & 9 mo. & 12 mo. \\ \midrule
3 mo. & 0.41 (0.36, 0.49) &  &  &  \\
6 mo. & 0.38 (0.32, 0.44) & 0.46 (0.40, 0.54) &  &  \\
9 mo. & 0.38 (0.32, 0.44) & 0.36 (0.31, 0.43) & 0.43 (0.37, 0.50) &  \\
12 mo. & 0.34 (0.29, 0.40) & 0.35 (0.30, 0.40) & 0.32 (0.12, 0.54) & 0.52 (0.44, 0.61) \\ \midrule
k = 2 & 3 mo. & 6 mo. & 9 mo. & 12 mo. \\ \midrule
3 mo. & 1.18 (1.01, 1.39) &  &  &  \\
6 mo. & 0.75 (0.56, 0.95) & 1.26 (1.11, 1.44) &  &  \\
9 mo. & 0.94 (0.77, 1.11) & 0.82 (0.68, 0.99) & 1.33 (1.16, 1.53) &  \\
12 mo. & 0.67 (0.52, 0.83) & 0.80 (0.67, 0.95) & 0.88 (0.74, 1.04) & 1.27 (1.12, 1.45) \\ \bottomrule
\end{tabular}
\end{table}

\clearpage

\section*{Web Appendix E: Web Figures}

\begin{figure}[h]
\caption{\label{fig:sim1_trace} Trace plots of a selection of parameters from Simulation 1. Geweke diagnostics and effective sample sizes (ESS) are shown for each parameter. MCMC sampling was run for 10000 iterations with a burn-in of 1000. All parameters were initialized at 0 and prior parameters were chosen to be weakly informative.}
\includegraphics[width = 1\textwidth]{sim1_beta_trace.jpg}
\end{figure}

\begin{figure}[h]
\caption{\label{fig:sim2_trace} Trace plots of a selection of parameters from the MNAR imputation model in Simulation 2. Geweke diagnostics and effective sample sizes (ESS) are shown for each parameter. MCMC sampling was run for 10000 iterations with a burn-in of 1000. All parameters were initialized at 0 and prior parameters were chosen to be weakly informative.}
\includegraphics[width = 1\textwidth]{sim2_delta_trace.jpg}
\end{figure}

\begin{figure}[h]
\caption{\label{fig:sim3_trace} Trace plots of a selection of parameters from the 3-class model in Simulation 3. Geweke diagnostics and effective sample sizes (ESS) are shown for each parameter. MCMC sampling was run for 10000 iterations with a burn-in of 1000. All parameters were initialized at 0 and prior parameters were chosen to be weakly informative.}
\includegraphics[width = 1\textwidth]{sim3_alpha_trace.jpg}
\end{figure}

\begin{figure}[h]
\caption{\label{fig:app_trace} Trace plots of a selection of parameters from the application to the Nurture Data. Geweke diagnostics and effective sample sizes (ESS) are shown for each parameter. MCMC sampling was run for 10000 iterations with a burn-in of 1000. All parameters were initialized at 0 and prior parameters were chosen to be weakly informative.}
\includegraphics[width = 1\textwidth]{app_trace.jpg}
\end{figure}

\clearpage

\textbf{References}:\\
\begin{enumerate}
\item Laurent, S. (2018). Matrixsampling: Simulations of Matrix Variate Distributions.
\item Geweke, J. (1992). Evaluating the accuracy of sampling-based approaches to calculating posterior moments. In {\em Bayesian Statistics 4 (ed JM Bernado, JO Berger, AP Dawid and AFM Smith)}. Clarendon Press, Oxford, UK.
\end{enumerate}
\end{document} 